{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets \nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.transforms import InterpolationMode\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport time \nimport os \n\nimport pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class PetDataset(torch.utils.data.Dataset):\n#     def __init__(self, root, split=\"train\"):\n#         self.dataset = datasets.OxfordIIITPet(\n#             root=root,\n#             target_types='segmentation', \n#             download=True\n#         )\n#         self.split = split\n#         # self.rotation = transforms.RandomChoice([\n#         #     transforms.RandomRotation((0, 0), interpolation=InterpolationMode.BILINEAR),\n#         #     transforms.RandomRotation((90, 90), interpolation=InterpolationMode.BILINEAR),\n#         #     transforms.RandomRotation((180, 180), interpolation=InterpolationMode.BILINEAR),\n#         #     transforms.RandomRotation((270, 270), interpolation=InterpolationMode.BILINEAR)\n#         # ])\n\n#         # Image transforms\n#         self.img_transform = transforms.Compose([\n#             transforms.Resize((128, 128)),\n#             # transforms.RandomHorizontalFlip(p=0.5),\n#             # transforms.RandomVerticalFlip(p=0.5),\n#             # self.rotation,\n#             transforms.ToTensor()\n#         ])\n#     def __getitem__(self, idx):\n#         img, mask = self.dataset[idx]\n#         img = self.img_transform(img)\n#         # mask = self.img_transform(mask)\n#         mask = mask.resize((128, 128), Image.NEAREST)\n#         mask = torch.from_numpy(np.array(mask)).long()\n\n#         mask[mask == 2] = 0\n#         mask[mask == 3] = 0\n#         mask[mask == 1] = 1  # Pet\n\n\n#         return img, mask\n\n#     def __len__(self):\n#         return len(self.dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import InterpolationMode\nimport random\nimport numpy as np\nfrom PIL import Image\n\n\nclass PetDataset(Dataset):\n    def __init__(self, root, split=\"train\"):\n        self.dataset = datasets.OxfordIIITPet(\n            root=root,\n            target_types='segmentation',\n            download=True\n        )\n        self.split = split\n\n        # Only resizing and ToTensor are safely reusable for images\n        self.resize = transforms.Resize((128, 128), interpolation=InterpolationMode.BILINEAR)\n        self.resize_mask = transforms.Resize((128, 128), interpolation=InterpolationMode.NEAREST)\n\n    def __getitem__(self, idx):\n        img, mask = self.dataset[idx]\n\n        # --------- Resize ---------------------------------\n        img = self.resize(img)\n        mask = self.resize_mask(mask)\n        if self.split == \"train\":\n            # --------- Random Horizontal Flip ------------------\n            if random.random() < 0.5:\n                img = F.hflip(img)\n                mask = F.hflip(mask)\n    \n            # --------- Random Vertical Flip --------------------\n            if random.random() < 0.5:\n                img = F.vflip(img)\n                mask = F.vflip(mask)\n    \n            # --------- Random 90-degree Rotations --------------\n            rotations = [0, 90, 180, 270]\n            angle = random.choice(rotations)\n            img = F.rotate(img, angle, interpolation=InterpolationMode.BILINEAR)\n            mask = F.rotate(mask, angle, interpolation=InterpolationMode.NEAREST)\n\n        # --------- Convert types ---------------------------\n        img = F.to_tensor(img)\n\n        mask = torch.from_numpy(np.array(mask)).long()\n\n        mask[mask == 2] = 0\n        mask[mask == 3] = 0\n        mask[mask == 1] = 1\n\n        return img, mask\n\n    def __len__(self):\n        return len(self.dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1), \n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1), \n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class UNet(nn.Module):\n#     def __init__(self, n_classes=1):\n#         super().__init__()\n#         self.d1 = DoubleConv(3, 64)\n#         self.d2 = DoubleConv(64, 128)\n#         self.d3 = DoubleConv(128, 256)\n#         self.u1 = DoubleConv(256+128, 128)\n#         self.u2 = DoubleConv(128+64, 64)\n#         self.out_conv = nn.Conv2d(64, 1, 1)\n#         self.pool = nn.MaxPool2d(2)\n#         self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        \n#         # Add weight initialization\n#         self._initialize_weights()\n\n#     def _initialize_weights(self):\n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n#                 if m.bias is not None:\n#                     nn.init.constant_(m.bias, 0)\n#             elif isinstance(m, nn.BatchNorm2d):\n#                 nn.init.constant_(m.weight, 1)\n#                 nn.init.constant_(m.bias, 0)\n\n#     def forward(self, x):\n#         x1 = self.d1(x)\n#         x2 = self.d2(self.pool(x1))\n#         x3 = self.d3(self.pool(x2))\n#         x = self.up(x3)\n#         x = self.u1(torch.cat([x, x2], dim=1))\n#         x = self.up(x)\n#         x = self.u2(torch.cat([x, x1], dim=1))\n#         out = self.out_conv(x)\n#         return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        self.d1 = DoubleConv(3, 64)\n        self.d2 = DoubleConv(64, 128)\n        self.d3 = DoubleConv(128, 256)\n        self.u1 = DoubleConv(256+128, 128)\n        self.u2 = DoubleConv(128+64, 64)\n        self.out_conv = nn.Conv2d(64, 1, 1)\n        self.pool = nn.MaxPool2d(2)\n        self.up = nn.Upsample(scale_factor = 2, mode = \"bilinear\", align_corners=True)\n        \n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x1 = self.d1(x)\n        x2 = self.d2(self.pool(x1))\n        x3 = self.d3(self.pool(x2))\n        x = self.up(x3)\n        x = self.u1(torch.cat([x, x2], dim=1))\n        x = self.up(x)\n        x = self.u2(torch.cat([x, x1], dim=1))\n        out = self.out_conv(x)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = PetDataset(\"./data\", split=\"train\")\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\nval_ds = PetDataset(\"./data\", split=\"val\")\nval_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory data analysis on the image(EDA)","metadata":{}},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/data/oxford-iiit-pet/images\")[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open(\"/kaggle/working/data/oxford-iiit-pet/images/wheaten_terrier_14.jpg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"This is the size of the original image: {img.size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Resize image to see the shape of the input to the network","metadata":{}},{"cell_type":"code","source":"transformations = transforms.Compose([\n    transforms.Resize((128, 128))\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_reshaped = transformations(img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"This is the size of the resized image: {img_reshaped.size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(img_reshaped)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA on the labels","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/working/data/oxford-iiit-pet/annotations/list.txt\", \"r\") as f:\n    content = f.read()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"content[:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/data/oxford-iiit-pet/annotations/test.txt\", \"r\") as f:\n    content = f.read()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"content[:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/data/oxford-iiit-pet/annotations/trainval.txt\", \"r\") as f:\n    content = f.read()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"content[:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/data/oxford-iiit-pet/annotations/trimaps\")[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open(\"/kaggle/working/data/oxford-iiit-pet/annotations/trimaps/British_Shorthair_165.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img.size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inspect the image values","metadata":{}},{"cell_type":"code","source":"img_array = np.array(img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(img_array)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum_1 = np.sum(img_array[img_array==1])\nsum_2 = np.sum(img_array[img_array==2])\nsum_3 = np.sum(img_array[img_array==3])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_flat = img_array.flatten()\n\n# Plot histogram\nplt.hist(mask_flat, bins=np.arange(5)-0.5, rwidth=0.8)\nplt.xticks([ 1, 2, 3])  # possible mask values\nplt.xlabel(\"Mask Value\")\nplt.ylabel(\"Number of pixels\")\nplt.title(\"Histogram of mask pixel values\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Viusalize only background, boarder and pet","metadata":{}},{"cell_type":"code","source":"img_array.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_1 = (img_array == 1)\n\nplt.imshow(mask_1, cmap='gray')\nplt.axis('off')\nplt.title(\"Pixels with value 1\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_2 = (img_array == 2)\n\nplt.imshow(mask_2, cmap='gray')\nplt.axis('off')\nplt.title(\"Pixels with value 2\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_3 = (img_array == 3)\n\nplt.imshow(mask_3, cmap='gray')\nplt.axis('off')\nplt.title(\"Pixels with value 3\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### So 1 belongs to the pet itself!!\n### 2 belongs to background \n### 3 belings to baorder","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, pred, target):\n        pred = torch.sigmoid(pred)\n        smooth = 1.0\n        intersection = (pred * target).sum()\n        dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n        return 1.0 - dice\n\n# Use this instead of BCE\ncriterion = DiceLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet().to(device)\nepochs = 30\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for imgs, masks in loader:\n        imgs, masks = imgs.to(device), masks.to(device)\n        masks = masks.float().unsqueeze(1)  # Shape [B,1,H,W]\n        \n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    return total_loss / len(loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for imgs, masks in loader:\n            imgs, masks = imgs.to(device), masks.to(device)\n            masks = masks.float().unsqueeze(1)\n            outputs = model(imgs)\n            loss = criterion(outputs, masks)\n            total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_loss = float('inf')\nfor epoch in range(epochs):\n    t0 = time.time()\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss = validate(model, val_loader, criterion, device)\n   \n    if val_loss < best_val_loss:\n        print(\"Best Result! Model will be saved\")\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"model.pt\")\n\n        \n    t1 = time.time()\n    elapsed_time = t1 - t0\n    print(f\"Epoch {epoch+1:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {elapsed_time:.2f}s\")\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet().to(device)\nmodel.load_state_dict(torch.load(\"model.pt\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    for imgs, masks in train_loader:\n\n        imgs, masks = imgs.to(device), masks.to(device)\n        preds = model(imgs)\n        preds = torch.sigmoid(preds)\n        preds = (preds > 0.5).float().cpu()\n\n        # Visualize results\n        img = imgs[10].cpu().numpy().transpose(1, 2, 0)\n        mask = masks[10].cpu().squeeze().numpy()\n        pred = preds[10].cpu().squeeze().numpy()\n        \n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 3, 1)\n        plt.imshow(img)\n        plt.title(\"Input Image\")\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 2)\n        plt.imshow(mask, cmap='gray')\n        plt.title(\"Ground Truth\")\n        plt.axis('off')\n        \n        plt.subplot(1, 3, 3)\n        plt.imshow(pred, cmap='gray')\n        plt.title(\"Prediction\")\n        plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}